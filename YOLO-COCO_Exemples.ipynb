{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATION DU MODELE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing yolo/cfg/yolo-lite.cfg\n",
      "Loading yolo/weights/yolo-lite.weights ...\n",
      "Successfully identified 2597684 bytes\n",
      "Finished in 0.0020074844360351562s\n",
      "\n",
      "Building net ...\n",
      "Source | Train? | Layer description                | Output size\n",
      "-------+--------+----------------------------------+---------------\n",
      "       |        | input                            | (?, 224, 224, 3)\n",
      " Load  |  Yep!  | conv 3x3p1_1    leaky            | (?, 224, 224, 16)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 112, 112, 16)\n",
      " Load  |  Yep!  | conv 3x3p1_1    leaky            | (?, 112, 112, 32)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 56, 56, 32)\n",
      " Load  |  Yep!  | conv 3x3p1_1    leaky            | (?, 56, 56, 64)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 28, 28, 64)\n",
      " Load  |  Yep!  | conv 3x3p1_1    leaky            | (?, 28, 28, 128)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 14, 14, 128)\n",
      " Load  |  Yep!  | conv 3x3p1_1    leaky            | (?, 14, 14, 128)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 7, 7, 128)\n",
      " Load  |  Yep!  | conv 3x3p1_1    leaky            | (?, 7, 7, 256)\n",
      " Load  |  Yep!  | conv 1x1p0_1    linear           | (?, 7, 7, 425)\n",
      "-------+--------+----------------------------------+---------------\n",
      "Running entirely on CPU\n",
      "Finished in 0.1380321979522705s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importation des librairies \n",
    "\n",
    "import cv2 # OpenCV\n",
    "from darkflow.net.build import TFNet # Darkflow\n",
    "\n",
    "# Specification des options du modele dans un dictionnaire\n",
    "\n",
    "options = {\"model\" : \"yolo/cfg/yolo-lite.cfg\", \"load\": \"yolo/weights/yolo-lite.weights\", \"threshold\" : 0.3}\n",
    "\n",
    "# Initialisation du modele\n",
    "# Reessayer si ca ne marche pas du premier coup. \n",
    "\n",
    "model = TFNet(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMAGES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importaton et affichage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les images sont dans le fichier data. \n",
    "# 4 images: cyclists.jpeg, dogcat.jpeg, dogs.jpeg, street.jpeg\n",
    "\n",
    "# On place les images dans des variables grace a la fonction OpenCV cv2.imread()\n",
    "\n",
    "# IMPORTANT: appuyer sur n'importe quelle touche, et non sur le x pour fermer la fenetre avec l'image.\n",
    "\n",
    "cyclists = cv2.imread(\"data/cyclists.jpeg\")\n",
    "cv2.imshow(\"cyclists\", cyclists)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "dogs = cv2.imread(\"data/dogs.jpeg\")\n",
    "cv2.imshow(\"dogs\", dogs)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "dogcat = cv2.imread(\"data/dogcat.jpeg\")\n",
    "cv2.imshow(\"dogcat\", dogcat)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "street = cv2.imread(\"data/street.jpeg\")\n",
    "cv2.imshow(\"street\", street)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(optionnel)rgb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(390, 553, 3)\n"
     ]
    }
   ],
   "source": [
    "# Importation de la librairie numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# La fonction np.shape() retourne les dimensions d'un tableau.\n",
    "# Appliquee a une image, elle retourne les dimensions en pixel (hauteur, largeur) et les trois dimensions RGB \n",
    "\n",
    "print(np.shape(cyclists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comme vu en classe, une image est constituee de trois images superposees\n",
    "# En mettant deux de ces images sur trois a 0 on peut afficher la composition RGB de chaque couleur \n",
    "\n",
    "B = np.copy(cyclists)\n",
    "B[:, :, 1] = 0\n",
    "B[:, :, 2] = 0\n",
    "\n",
    "cv2.imshow(\"Blue\", B)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.copy(cyclists)\n",
    "G[:,:,0] = 0\n",
    "G[:,:,2] = 0\n",
    "\n",
    "cv2.imshow(\"Green\", G)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.copy(cyclists)\n",
    "R[:,:,0] = 0\n",
    "R[:,:,1] = 0\n",
    "    \n",
    "cv2.imshow(\"Red\", R)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**yolo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'person', 'confidence': 0.6257671, 'topleft': {'x': 251, 'y': 165}, 'bottomright': {'x': 291, 'y': 237}}, {'label': 'person', 'confidence': 0.73720366, 'topleft': {'x': 220, 'y': 165}, 'bottomright': {'x': 336, 'y': 338}}, {'label': 'person', 'confidence': 0.891898, 'topleft': {'x': 383, 'y': 175}, 'bottomright': {'x': 497, 'y': 324}}, {'label': 'bicycle', 'confidence': 0.80077744, 'topleft': {'x': 50, 'y': 175}, 'bottomright': {'x': 161, 'y': 328}}]\n"
     ]
    }
   ],
   "source": [
    "# On utilise la fonction return_predict de l'objet model sur nos images. Elle retourne un array de dictionnaires qu'on range dans une variable\n",
    "# et qu'on imprime a l'ecran. \n",
    "\n",
    "cyclists_detection = model.return_predict(cyclists)\n",
    "print(cyclists_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'dog', 'confidence': 0.31806296, 'topleft': {'x': 171, 'y': 43}, 'bottomright': {'x': 271, 'y': 137}}, {'label': 'sheep', 'confidence': 0.3068601, 'topleft': {'x': 142, 'y': 36}, 'bottomright': {'x': 251, 'y': 140}}]\n"
     ]
    }
   ],
   "source": [
    "dogs_detection = model.return_predict(dogs)\n",
    "print(dogs_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'cat', 'confidence': 0.4078258, 'topleft': {'x': 93, 'y': 82}, 'bottomright': {'x': 265, 'y': 147}}, {'label': 'cow', 'confidence': 0.3400079, 'topleft': {'x': 22, 'y': 36}, 'bottomright': {'x': 180, 'y': 144}}]\n"
     ]
    }
   ],
   "source": [
    "dogcat_detection = model.return_predict(dogcat)\n",
    "print(dogcat_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'person', 'confidence': 0.4163806, 'topleft': {'x': 1458, 'y': 519}, 'bottomright': {'x': 1503, 'y': 617}}, {'label': 'car', 'confidence': 0.45718357, 'topleft': {'x': 299, 'y': 604}, 'bottomright': {'x': 403, 'y': 764}}, {'label': 'person', 'confidence': 0.83188576, 'topleft': {'x': 1440, 'y': 514}, 'bottomright': {'x': 1541, 'y': 830}}, {'label': 'person', 'confidence': 0.46588147, 'topleft': {'x': 695, 'y': 677}, 'bottomright': {'x': 844, 'y': 928}}, {'label': 'person', 'confidence': 0.47984228, 'topleft': {'x': 1456, 'y': 696}, 'bottomright': {'x': 1566, 'y': 963}}, {'label': 'traffic light', 'confidence': 0.4171699, 'topleft': {'x': 790, 'y': 563}, 'bottomright': {'x': 827, 'y': 602}}, {'label': 'car', 'confidence': 0.51545095, 'topleft': {'x': 1009, 'y': 554}, 'bottomright': {'x': 1050, 'y': 598}}, {'label': 'car', 'confidence': 0.4044988, 'topleft': {'x': 60, 'y': 579}, 'bottomright': {'x': 186, 'y': 795}}, {'label': 'car', 'confidence': 0.7800603, 'topleft': {'x': 521, 'y': 596}, 'bottomright': {'x': 630, 'y': 728}}, {'label': 'car', 'confidence': 0.5327976, 'topleft': {'x': 714, 'y': 582}, 'bottomright': {'x': 852, 'y': 759}}, {'label': 'car', 'confidence': 0.70025986, 'topleft': {'x': 964, 'y': 573}, 'bottomright': {'x': 1112, 'y': 732}}, {'label': 'car', 'confidence': 0.6958524, 'topleft': {'x': 1198, 'y': 577}, 'bottomright': {'x': 1333, 'y': 729}}, {'label': 'traffic light', 'confidence': 0.6812626, 'topleft': {'x': 577, 'y': 347}, 'bottomright': {'x': 609, 'y': 414}}, {'label': 'traffic light', 'confidence': 0.33331287, 'topleft': {'x': 780, 'y': 382}, 'bottomright': {'x': 810, 'y': 433}}, {'label': 'traffic light', 'confidence': 0.31858683, 'topleft': {'x': 1032, 'y': 392}, 'bottomright': {'x': 1065, 'y': 431}}, {'label': 'traffic light', 'confidence': 0.43770486, 'topleft': {'x': 1252, 'y': 365}, 'bottomright': {'x': 1286, 'y': 424}}, {'label': 'traffic light', 'confidence': 0.34101346, 'topleft': {'x': 1425, 'y': 337}, 'bottomright': {'x': 1464, 'y': 419}}, {'label': 'traffic light', 'confidence': 0.5073879, 'topleft': {'x': 561, 'y': 554}, 'bottomright': {'x': 603, 'y': 601}}, {'label': 'skateboard', 'confidence': 0.42262745, 'topleft': {'x': 1473, 'y': 896}, 'bottomright': {'x': 1581, 'y': 1043}}]\n"
     ]
    }
   ],
   "source": [
    "street_detection = model.return_predict(street)\n",
    "print(street_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fonction pour bounding boxes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specification d'une fonction pour dessiner les boites englobantes. \n",
    "# Elle prend en argument la sortie de YOLO (results) et l'image sur laquelle on dessine (img)\n",
    "\n",
    "import math\n",
    "\n",
    "def bounding_boxes(results, img):\n",
    "    # Une boucle traverse toutes les detections. \n",
    "\tfor result in results:\n",
    "\t\t# Parse detector output\n",
    "\t\tlabel = result[\"label\"]\n",
    "\t\tconfidence = result[\"confidence\"]\n",
    "\n",
    "\t\txtopleft = result[\"topleft\"][\"x\"]\n",
    "\t\tytopleft = result[\"topleft\"][\"y\"]\n",
    "\t\t\n",
    "\t\txbottomright = result[\"bottomright\"][\"x\"]\n",
    "\t\tybottomright = result[\"bottomright\"][\"y\"]\n",
    "\n",
    "\t\tlength = xbottomright - xtopleft\n",
    "\t\theight = ybottomright - ytopleft\n",
    "\n",
    "\t\t# Une fonction Open CV nous permet de dessiner sur l'image. \t\t\n",
    "\t\tcv2.rectangle(img, (xtopleft, ytopleft), (xbottomright, ybottomright), (255, 0, 0), 2)\n",
    "\t\tcv2.putText(img, \"Label: \" + label, (xtopleft, ytopleft - math.floor(0.05*height)), cv2.FONT_HERSHEY_SIMPLEX, 0.005*length, (255, 0, 0))\n",
    "\t\tcv2.putText(img, \"Confidence:\" + str(confidence), (xtopleft, ytopleft + math.floor(0.12*height)), cv2.FONT_HERSHEY_SIMPLEX, 0.005*length, (255, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalement, on affiche l'image a l'ecran, mais cette fois avec les boites englobantes correspondant aux detections\n",
    "# Discuter un peu les resultats avec les etudiants. Quel genre de detection? Comment expliquer les erreurs; animaux de profil, image floue ou objet obstrue, etc.\n",
    "\n",
    "bounding_boxes(cyclists_detection, cyclists)\n",
    "\n",
    "cv2.imshow(\"cyclists\", cyclists)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_boxes(dogs_detection, dogs)\n",
    "\n",
    "cv2.imshow(\"dogs\", dogs)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_boxes(dogcat_detection, dogcat)\n",
    "\n",
    "cv2.imshow(\"dogcat\", dogcat)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_boxes(street_detection, street)\n",
    "\n",
    "cv2.imshow(\"dogcat\", street)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VIDEO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importation et affichage** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objet capture video OpenCV\n",
    "\n",
    "pedestrians = cv2.VideoCapture(\"data/pedestrians.avi\")\n",
    "\n",
    "# Tant qu'il reste des frames dans le video, l'afficher a l'ecran\n",
    "\n",
    "while pedestrians.isOpened():\n",
    "    \n",
    "    # On obtient le frame le plus recent grace a la fonction read() de l'objet capture pedestrians \n",
    "    \n",
    "    ret, frame = pedestrians.read()\n",
    "    \n",
    "    # Si le frame est vide, sortir de la boucle \n",
    "    \n",
    "    if frame is None:\n",
    "        break\n",
    "    \n",
    "    # Afficher l'image a l'ecran \n",
    "    \n",
    "    cv2.imshow(\"pedestrians\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "# Relacher la capture et fermer la fenetre \n",
    "\n",
    "pedestrians.release()\n",
    "cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**yolo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meme boucle que dans l'exemple precedent, a ceci pres qu'on applique YOLO a chaque frame et qu'on dessine les boites englobantes a l'ecran\n",
    "\n",
    "pedestrians = cv2.VideoCapture(\"data/pedestrians.avi\")\n",
    "\n",
    "while pedestrians.isOpened():\n",
    "    \n",
    "    # Lis frame le plus recent\n",
    "    \n",
    "    ret, frame = pedestrians.read()\n",
    "    \n",
    "    if frame is None:\n",
    "        break\n",
    "    \n",
    "    # Application de YOLO \n",
    "    \n",
    "    predictons = model.return_predict(frame)\n",
    "    \n",
    "    # Tracer les boites englobantes\n",
    "    \n",
    "    bounding_boxes(predictons, frame)\n",
    "    \n",
    "    # Afficher l'image modifiee a l'ecran\n",
    "    \n",
    "    cv2.imshow(\"pedestrians\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "pedestrians.release()\n",
    "cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optionnel)**WEBCAM**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
